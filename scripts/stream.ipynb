{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "import gc\n",
    "\n",
    "class ChatLlamaCppManager(ChatLlamaCpp):\n",
    "  def __init__(self, model_path, **kwargs):\n",
    "    super().__init__(model_path=model_path, **kwargs)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab255c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Response:\n",
      "George Washington is considered the first president of the United States. He served two terms in office, from April 30, 1789 to March 4, 1797.\n",
      "\n",
      "Washington was unanimously elected by the Electoral College for both his terms as President. Before taking up this post, he had played a crucial role during America's Revolutionary War."
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stderr\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "log_path = \"/Users/deepesh/Desktop/github-projects/ai-agent/llama_cpp_backend.log\"\n",
    "\n",
    "with open(log_path, \"w\") as log_file, redirect_stderr(log_file):\n",
    "    model = ChatLlamaCppManager(\n",
    "        model_path=\"/Users/deepesh/llms/mistral-nemo-15.gguf\",\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        n_ctx=2048,\n",
    "        repeat_penalty=1.2,\n",
    "        n_gpu_layers=-1,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "    # Define a function that calls model\n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    # Define a node in the graph\n",
    "    workflow.add_edge(START, \"model\")\n",
    "    workflow.add_node(\"model\", call_model)\n",
    "\n",
    "    # Add memory\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    config: RunnableConfig = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "    query = \"Who was the first president of the United States?\"\n",
    "    input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "    print(\"ðŸ§  Response:\")\n",
    "    for chunk, metadata in app.stream({\"messages\": input_messages}, config=config, stream_mode=\"messages\"):\n",
    "        if isinstance(chunk, AIMessage):\n",
    "            print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f773befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Response:\n",
      "George Washington is considered the first president of the United States. He served two terms in office, from April 30, 1789 to March 4, 1797.\n",
      "\n",
      "Washington was unanimously elected by the Electoral College for both his terms as President. Before taking up this post, he had played a crucial role during America's Revolutionary War."
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stderr\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "log_path = \"/Users/deepesh/Desktop/github-projects/ai-agent/llama_cpp_backend.log\"\n",
    "\n",
    "with open(log_path, \"w\") as log_file, redirect_stderr(log_file):\n",
    "    model = ChatLlamaCpp(\n",
    "        model_path=\"/Users/deepesh/llms/mistral-nemo-15.gguf\",\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        n_ctx=2048,\n",
    "        repeat_penalty=1.2,\n",
    "        n_gpu_layers=-1,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "    # Define a function that calls model\n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    # Define a node in the graph\n",
    "    workflow.add_edge(START, \"model\")\n",
    "    workflow.add_node(\"model\", call_model)\n",
    "\n",
    "    # Add memory\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    config: RunnableConfig = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n",
    "\n",
    "    query = \"Who was the first president of the United States?\"\n",
    "    input_messages = [HumanMessage(content=query)]\n",
    "\n",
    "    print(\"ðŸ§  Response:\")\n",
    "    for chunk, metadata in app.stream({\"messages\": input_messages}, config=config, stream_mode=\"messages\"):\n",
    "        if isinstance(chunk, AIMessage):\n",
    "            print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
